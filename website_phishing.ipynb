{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konu: Website Phishing: Çeşitli web sitelerinden toplanan 9 özniteliğe göre 1353 web \n",
    "sitesinin kimlik avı-dolandırıcılık(pishing) tehditlerine göre Meşru:‘1’, Şüpheli: \n",
    "‘0’, Dolandırıcı(pishing):’-1’ sınıflandırılmıştır. \n",
    "Algoritma:  \n",
    "LR(LogisticRegression: Lojistik regresyon tabanlı sınıflandırma) \n",
    "KNC(KNeighborsClassifier: K-en yakın komşular sınıflandırma) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kütüphanelerin Eklenmesi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Bilimsel hesaplama için. Yani diziler, matrisler, matematiksel işlemler diyebiliriz.\n",
    "import pandas as pd #veri analizi için\n",
    "import matplotlib.pyplot as plt #görselleştirme için kullanacağız\n",
    "from sklearn.model_selection import train_test_split #veriyi eğitim ve test verilerine ayırmamızı sağlayacak \n",
    "from sklearn.preprocessing import StandardScaler # veri ölçeklendirme yani normalizasyon işlemi için kullanacağız\n",
    "from sklearn.linear_model import LogisticRegression # LR modelini oluşturmamız için bu kütüphaneyi eklememiz gerek.\n",
    "#veri performans ölçmek için birden fazla yolu aşağıdaki komut ile ekliyoruz. Ör: f1 scoru gibi \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize # multiclass yapıyı binary formata dönüştürmek için kullanılıyor. \n",
    "from itertools import cycle #Döngü oluşturur ve renkler atar. \n",
    "from scipy.io import arff #veri dosyasının yapısı yüzünden ilk seferde okuma işlemini gerçekleştiremedim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verinin Okunması "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Veri dosyasını okuyup hakkında bilgi alarak başlayacağız. Ardından bu verileri kontrol edip ona göre hareket edeceğiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satır sayısı: 2\n",
      "Sütun sayısı: 1353\n",
      "Veri Setinin Bir Kısmı: \n",
      "                                                0     ...                                               1352\n",
      "0  [b'1', b'-1', b'1', b'-1', b'-1', b'1', b'1', ...  ...  [b'1', b'0', b'1', b'1', b'1', b'0', b'-1', b'...\n",
      "1                                                SFH  ...                                               None\n",
      "\n",
      "[2 rows x 1353 columns]\n",
      "\n",
      "Veri Seti Hakkında Bilgi:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Columns: 1353 entries, 0 to 1352\n",
      "dtypes: object(1353)\n",
      "memory usage: 21.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#.arff dosyas okuma \n",
    "data_arff = arff.loadarff(\"data/PhishingData.arff\")\n",
    "\n",
    "#veri ile işlem yapabilmeye devam etmem için öncelikle pd ye uyumlu hale getirmeliyim \n",
    "data = pd.DataFrame(data_arff)\n",
    "\n",
    "# Satır ve sütun sayısını kontrol et\n",
    "print(\"Satır sayısı:\", data.shape[0])\n",
    "print(\"Sütun sayısı:\", data.shape[1])\n",
    "\n",
    "print(\"Veri Setinin Bir Kısmı: \")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nVeri Seti Hakkında Bilgi:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksik Değer Kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Eksik Değer Kontrolü Yapalım:\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1348    1\n",
      "1349    1\n",
      "1350    1\n",
      "1351    1\n",
      "1352    1\n",
      "Length: 1353, dtype: int64\n",
      "\n",
      "Tamamen NaN olan sütunlar: Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Eksik Değer Kontrolü Yapalım:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Tamamen NaN olan sütunları kontrol et\n",
    "nan_columns = data.columns[data.isnull().all()]\n",
    "print(\"\\nTamamen NaN olan sütunlar:\", nan_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Çıktıya göre boş sütun yok o yüzden kaldırmamız gereken bir sütunda yok demektir. Olsaydı duruma el atmamız lazımdı şekerim <3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Çıktı yorumlarsak veri setindeki veriler object türünde. Eksik verileri tamamlamak istediğimizde veri türlerini değiştirmemiz gerekecek yoksa bu kısım ileride hata almamıza sebep olacak. \n",
    "- Eksik değer kontrolünde ilk satırlarda eksik değer gözükmüyor fakat sonraki 1348. satırdan itibaren satırlarda eksik görünmeye başlanmış ve bu 1 ile ifade edilmiş. Eksik satırlar doldurulmalı ki model düşük performans ile çalışmasın. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Sayısallaştırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Veri Türleri:\n",
      "0       float64\n",
      "1       float64\n",
      "2       float64\n",
      "3       float64\n",
      "4       float64\n",
      "         ...   \n",
      "1348    float64\n",
      "1349    float64\n",
      "1350    float64\n",
      "1351    float64\n",
      "1352    float64\n",
      "Length: 1353, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Object Türündeki sütunları listeleyip bir diziye atayacağım. Ardından dönüşüm yapacağım\n",
    "non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Object türündeki sütunlarda temizleme işlemi\n",
    "for col in non_numeric_columns:\n",
    "    data[col] = data[col].str.strip()  # Başındaki ve sonundaki boşlukları temizle\n",
    "    \n",
    "# Sayısallaştırma işlemi\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"\\nVeri Türleri:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksik Veri Tamamlama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Doldurma İşleminden Sonra Eksik Değer Kontrolü:\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1348    0\n",
      "1349    0\n",
      "1350    0\n",
      "1351    0\n",
      "1352    0\n",
      "Length: 1353, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#eksik değerleri 0 ile dolduralım. Çünkü ne suçlu ne meşru diyebiliriz. 0 şüpheli iyidir :)\n",
    "# ilk denemem de fillna('0') olarak yazdığım için sayısallaştırma işlemi yine tersine döndü :D \n",
    "data = data.fillna(0) \n",
    "\n",
    "#sonrasında bir kez daha değerleri kontrol edelim \n",
    "print(\"\\n Doldurma İşleminden Sonra Eksik Değer Kontrolü:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Önişlem Yapılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri seti boyutu: (2, 1353)\n",
      "\n",
      " x: (2, 1352)\n",
      "\n",
      " y (2,)\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:, :-1] #son sütun hariç tüm sütunlar\n",
    "y = data.iloc[:, -1] #son sütun \n",
    "\n",
    "print(\"Veri seti boyutu:\", data.shape)\n",
    "print(\"\\n x:\", x.shape)\n",
    "print(\"\\n y\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eğitim ve Test Verilerine Ayrılması "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eğitim Verileri: (1, 1352) (1, 1352)\n",
      "\n",
      "Test Verileri: (1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "#veriyi eğitim ve test verileri olmak üzere iki kısma böleceğiz. \n",
    "#genelde bunun için 0.2 veya 0.3 kullanılır\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Şimdi verileri kontrol edelim\n",
    "print(\"\\nEğitim Verileri:\", x_train.shape, x_test.shape)\n",
    "print(\"\\nTest Verileri:\", y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri Ölçeklendirme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizasyon işlemi yapacağız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Modeli Oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bu modeli kullanmak için bir kütüphane eklemiştik zaten. Bunu bize scikit-learn kütüphenesi sağladı. O yüzden biz ondan faydalanacağız sadece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.float64(0.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[238], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.float64(0.0)"
     ]
    }
   ],
   "source": [
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(x_train_scaled, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
